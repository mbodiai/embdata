{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"embodied data (Experimental)","text":"<p>Visualize, transform, clean, any type of unstructured multimodal data instantly.</p> <p>This library enables the vast majority of data processing, visualization, and analysis to be done in a single line of code with minimal dependencies. It is designed to be used in conjunction with rerun.io for visualizing complex data structures and trajectories and LeRobot for robotics simulations and training. See embodied-agents for real-world usage.</p> <p></p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>embodied data (Experimental)<ul> <li>Table of Contents</li> <li>Installation</li> <li>Quick Examples<ul> <li>Episode</li> <li>Geometry</li> </ul> </li> <li>Usage</li> <li>License</li> <li>Design Decisions</li> <li>Classes<ul> <li>Episode</li> <li>Image</li> <li>Sample</li> <li>Trajectory</li> <li>Motion<ul> <li>Key Concepts</li> </ul> </li> <li>AnyMotionControl</li> <li>HandControl</li> <li>AbsoluteHandControl</li> <li>RelativePoseHandControl</li> <li>HeadControl</li> <li>MobileSingleHandControl</li> <li>MobileBimanualArmControl</li> <li>HumanoidControl</li> </ul> </li> <li>Data Manipulation Definitions</li> <li>RL Definitions</li> </ul> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install embdata\n</code></pre> <p>Optionally, clone lerobot (https://github.com/huggingface/lerobot) and install:</p> <pre><code>git clone https://github.com/mbodiai/lerobot # or use our fork to install all dependencies at once.\ncd lerobot\npip install -e .\n</code></pre>"},{"location":"#quick-examples","title":"Quick Examples","text":"<p>Hover with intellisense or view the table of contents to see documentation for each class and method.</p> <ul> <li>The <code>Episode</code> class provides a list-like interface for a sequence of observations, actions, and/or other data. It's designed to streamline exploratory data analysis and manipulation of time series data.</li> <li>The <code>trajectory</code> method extracts a trajectory from the episode for a specified field, and enables easy visualization and analysis of the data, resampling with different frequencies, and filtering operations, and rescaling/normalizing the data.</li> <li>The <code>show</code> method visualizes the episode with rerun.io, a platform for visualizing 3D geometrical data, images, and graphs.</li> <li>The <code>Sample</code> class is a dict-like interface for serializing, recording, and manipulating arbitrary data. It provides methods for flattening and unflattening nested structures, converting between different formats, and integrating with machine learning frameworks and gym spaces.</li> </ul>"},{"location":"#episode","title":"Episode","text":"<pre><code>from embdata import Episode, TimeStep, ImageTask, Image, Motion, VisionMotorEpisode\nfrom embdata.geometry import Pose6D\nfrom embdata.motion.control import MobileSingleHandControl, HandControl, HeadControl\nfrom datasets import get_dataset_config_info, get_dataset_config_names, load_dataset\nfrom embdata.describe import describe\nfrom embdata.episode import Episode, VisionMotorEpisode\nfrom embdata.sample import Sample\n\n# Method 1: Create an episode from a HuggingFace dataset\nds = load_dataset(\"mbodiai/oxe_bridge_v2\", split=\"train\").take(10)\ndescribe(ds)\nds = Sample(ds)\nobs, actions, states = ds.flatten(to=\"observation\"), ds.flatten(to=\"action\"), ds.flatten(to=\"state\")\nzipped = zip(obs, actions, states, strict=False)\nepisode = VisionMotorEpisode(steps=zipped, freq_hz=5, observation_key=\"observation\", action_key=\"action\", state_key=\"state\")\nepisode.show(mode=\"local\") # Visualize the episode with rerun.io\n\n# Method 2: Create an episode from separate lists of observations, actions, and states\nobservations = [{\"image\": ..., \"task\": \"task1\", \"depth\": ...},\n                {\"image\": ..., \"task\": \"task2\"},\n                {\"image\": , \"task\": \"task2\"}]\nactions = [Motion(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n           BimanualArmControl(joint_angles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], ...)]\n           AnyMotionControl(velocity=0.1, joint_angles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6])]\nstates = [{\"scene_objects\": ..., \"reward\": 0},\n          {\"scene_objects\": ..., \"reward\": 1}]\nepisode2 = Episode(zip(observations, actions, states))\n\n\n# Method 3: Create an episode from a single list of dicts of any structure\nsteps = [\n    {\"observation\": {\"image\": np.random.rand(224, 224, 3), \"task\": \"pick\"},\n     \"action\": {\"position\": [0.1, 0.2, 0.3], \"orientation\": [0, 0, 0, 1]}},\n    {\"observation\": {\"image\": np.random.rand(224, 224, 3), \"task\": \"place\"},\n     \"action\": {\"position\": [0.4, 0.5, 0.6], \"orientation\": [0, 1, 0, 0]}}\n]\nepisode3 = Episode(steps)\n\n# Convert to LeRobot dataset\nlerobot_dataset = episode1.lerobot()\n\n# Convert from LeRobot dataset back to Episode\nepisode_from_lerobot = Episode.from_lerobot(lerobot_dataset)\n\n# Visualize the episode with rerun\nepisode1.show(mode=\"local\")\n# Iterate over steps\nfor step in episode.iter():\n    print(f\"Task: {step.observation.task}, Action: {step.action.position}\")\n\n# Extract trajectory\naction_trajectory = episode.trajectory(field=\"action\", freq_hz=10)\n\n# Visualize episode\nepisode.show(mode=\"local\")\n</code></pre>"},{"location":"#geometry","title":"Geometry","text":"<pre><code>from embdata.geometry import Pose6D\nimport numpy as np\n\n# Create a 6D pose\npose = Pose6D(x=1.0, y=2.0, z=3.0, roll=0.1, pitch=0.2, yaw=0.3)\n\n# Convert to different units\npose_cm = pose.to(\"cm\")\npose_deg = pose.to(angular_unit=\"deg\")\n\n# Get quaternion representation\nquat = pose.to(\"quaternion\")\n\n# Get rotation matrix\nrot_matrix = pose.to(\"rotation_matrix\")\n</code></pre>"},{"location":"#usage","title":"Usage","text":"Sample <p>The <code>Sample</code> class is a flexible base model for serializing, recording, and manipulating arbitrary data.</p> <p>Key features</p> <ul> <li>Serialization and deserialization of complex data structures</li> <li>Flattening and unflattening of nested structures</li> <li>Conversion between different formats (e.g., dict, numpy arrays, torch tensors)</li> <li>Integration with machine learning frameworks and gym spaces</li> </ul> <p>Usage example</p> <pre><code>from embdata import Sample\n\n# Create a simple Sample\nsample = Sample(x=1, y=2, z={\"a\": 3, \"b\": 4})\n\n# Flatten the sample\nflat_sample = sample.flatten()\nprint(flat_sample)  # [1, 2, 3, 4]\n\n# Flatten to a nested field\nnested_sample = Sample(x=1, y=2, z=[{\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6}])\na_fields = nested_sample.flatten(to=\"a\")  # [3, 5]\n\n# Convert to different formats\nas_dict = sample.to(\"dict\")\nas_numpy = sample.numpy()\nas_torch = sample.torch()\n\n# Create a random sample based on the structure\nrandom_sample = sample.random_sample()\n\n# Get the corresponding Gym space\nspace = sample.space()\n\n# Read a Sample from JSON or dictionary\nsample_from_json = Sample.read('{\"x\": 1, \"y\": 2}')\n\n# Get default value and space\ndefault_sample = Sample.default_value()\ndefault_space = Sample.default_space()\n\n# Get model information\nmodel_info = sample.model_info()\n\n# Pack and unpack samples\nsamples = [Sample(a=1, b=2), Sample(a=3, b=4)]\npacked = Sample.pack_from(samples)\nunpacked = packed.unpack()\n\n# Convert to HuggingFace Dataset and Features\ndataset = sample.dataset()\nfeatures = sample.features()\n</code></pre> <p>Methods</p> <ul> <li><code>flatten()</code>: Flattens the nested structure into a 1D representation</li> <li><code>unflatten()</code>: Reconstructs the original nested structure from a flattened representation</li> <li><code>to(format)</code>: Converts the sample to different formats (dict, numpy, torch, etc.)</li> <li><code>random_sample()</code>: Creates a random sample based on the current structure</li> <li><code>space()</code>: Returns the corresponding Gym space for the sample</li> <li><code>read()</code>: Reads a Sample instance from a JSON string, dictionary, or path</li> <li><code>default_value()</code>: Gets the default value for the Sample instance</li> <li><code>default_space()</code>: Returns the Gym space for the Sample class based on its class attributes</li> <li><code>model_info()</code>: Gets the model information</li> <li><code>unpack_from()</code>: Packs a list of samples into a single sample with lists for attributes</li> <li><code>pack_from()</code>: Unpacks the packed Sample object into a list of Sample objects or dictionaries</li> <li><code>dataset()</code>: Converts the Sample instance to a HuggingFace Dataset object</li> <li><code>features()</code>: Converts the Sample instance to a HuggingFace Features object</li> <li><code>lerobot()</code>: Converts the Sample instance to a LeRobot dataset</li> <li><code>space_for()</code>: Default Gym space generation for a given value</li> <li><code>init_from()</code>: Initializes a Sample instance from various data types</li> <li><code>from_space()</code>: Generates a Sample instance from a Gym space</li> <li><code>field_info()</code>: Gets the extra json values set from a FieldInfo for a given attribute key</li> <li><code>default_sample()</code>: Generates a default Sample instance from its class attributes</li> <li><code>numpy()</code>: Converts the Sample instance to a numpy array</li> <li><code>tolist()</code>: Converts the Sample instance to a list</li> <li><code>torch()</code>: Converts the Sample instance to a PyTorch tensor</li> <li><code>json()</code>: Converts the Sample instance to a JSON string</li> </ul> <p>The <code>Sample</code> class provides a wide range of functionality for data manipulation, conversion, and integration with various libraries and frameworks.</p> MobileSingleHandControl <p>The <code>MobileSingleHandControl</code> class represents control for a robot that can move its base in 2D space with a 6D EEF control and grasp.</p> <p>Usage Example</p> <pre><code>from embdata.geometry import PlanarPose\nfrom embdata.motion.control import MobileSingleHandControl, HandControl, HeadControl\n\n# Create a MobileSingleHandControl instance\ncontrol = MobileSingleHandControl(\n    base=PlanarPose(x=1.0, y=2.0, theta=0.5),\n    hand=HandControl(\n        pose=Pose(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n        grasp=0.5\n    ),\n    head=HeadControl(tilt=-0.1, pan=0.2)\n)\n\n# Access and modify the control\nprint(control.base.x)  # Output: 1.0\ncontrol.hand.grasp = 0.8\nprint(control.hand.grasp)  # Output: 0.8\n</code></pre> HumanoidControl <p>The <code>HumanoidControl</code> class represents control for a humanoid robot.</p> <p>Usage Example</p> <pre><code>import numpy as np\nfrom embdata.motion.control import HumanoidControl, HeadControl\n\n# Create a HumanoidControl instance\ncontrol = HumanoidControl(\n    left_arm=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]),\n    right_arm=np.array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7]),\n    left_leg=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]),\n    right_leg=np.array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7]),\n    head=HeadControl(tilt=-0.1, pan=0.2)\n)\n\n# Access and modify the control\nprint(control.left_arm)  # Output: [0.1 0.2 0.3 0.4 0.5 0.6]\ncontrol.head.tilt = -0.2\nprint(control.head.tilt)  # Output: -0.2\n</code></pre> Subclassing Motion <p>You can create custom motion controls by subclassing the <code>Motion</code> class.</p> <p>Usage Example</p> <pre><code>from embdata.motion import Motion\nfrom embdata.motion.fields import VelocityMotionField, AbsoluteMotionField\n\nclass CustomRobotControl(Motion):\n    linear_velocity: float = VelocityMotionField(default=0.0, bounds=[-1.0, 1.0])\n    angular_velocity: float = VelocityMotionField(default=0.0, bounds=[-1.0, 1.0])\n    arm_position: float = AbsoluteMotionField(default=0.0, bounds=[0.0, 1.0])\n\n# Create an instance of the custom control\ncustom_control = CustomRobotControl(\n    linear_velocity=0.5,\n    angular_velocity=-0.2,\n    arm_position=0.7\n)\n\nprint(custom_control)\n# Output: CustomRobotControl(linear_velocity=0.5, angular_velocity=-0.2, arm_position=0.7)\n\n# Validate bounds\ntry:\n    invalid_control = CustomRobotControl(linear_velocity=1.5)  # This will raise a ValueError\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n</code></pre> Image <p>The <code>Image</code> class represents image data and provides methods for manipulation and conversion.</p> <p>Key Features</p> <ul> <li>Multiple representation formats (NumPy array, base64, file path, PIL Image, URL)</li> <li>Easy conversion between different image formats</li> <li>Resizing and encoding capabilities</li> <li>Integration with other data processing pipelines</li> </ul> <p>Usage Example</p> <pre><code>from embdata import Image\nimport numpy as np\n\n# Create an Image from a numpy array\narray_data = np.random.rand(100, 100, 3)\nimg = Image(array=array_data)\n\n# Convert to base64\nbase64_str = img.base64\n\n# Open an image from a file\nimg_from_file = Image.open(\"path/to/image.jpg\")\n\n# Resize the image\nresized_img = Image(img_from_file, size=(50, 50))\n\n# Save the image\nimg.save(\"output_image.png\")\n\n# Create an Image from a URL\nimg_from_url = Image(\"https://example.com/image.jpg\")\n\n# Create an Image from a base64 string\nimg_from_base64 = Image.from_base64(base64_str, encoding=\"png\")\n</code></pre> <p>Methods</p> <ul> <li><code>open(path)</code>: Opens an image from a file path</li> <li><code>save(path, encoding, quality)</code>: Saves the image to a file</li> <li><code>show()</code>: Displays the image using matplotlib</li> <li><code>from_base64(base64_str, encoding, size, make_rgb)</code>: Creates an Image instance from a base64 string</li> <li><code>load_url(url, download)</code>: Downloads an image from a URL or decodes it from a base64 data URI</li> <li><code>from_bytes(bytes_data, encoding, size)</code>: Creates an Image instance from a bytes object</li> <li><code>space()</code>: Returns the space of the image</li> <li><code>dump(*args, as_field, **kwargs)</code>: Returns a dict or a field of the image</li> <li><code>infer_features_dict()</code>: Infers features of the image</li> </ul> <p>Properties</p> <ul> <li><code>array</code>: The image as a NumPy array</li> <li><code>base64</code>: The image as a base64 encoded string</li> <li><code>path</code>: The file path of the image</li> <li><code>pil</code>: The image as a PIL Image object</li> <li><code>url</code>: The URL of the image</li> <li><code>size</code>: The size of the image as a (width, height) tuple</li> <li><code>encoding</code>: The encoding format of the image</li> </ul> <p>Class Methods</p> <ul> <li><code>supports(arg)</code>: Checks if the argument is supported by the Image class</li> <li><code>pil_to_data(image, encoding, size, make_rgb)</code>: Creates an Image instance from a PIL image</li> <li><code>bytes_to_data(bytes_data, encoding, size, make_rgb)</code>: Creates an Image instance from a bytes object</li> </ul> <p>The <code>Image</code> class provides a convenient interface for working with image data in various formats and performing common image operations.</p> Trajectory <p>The <code>Trajectory</code> class represents a time series of multidimensional data, such as robot movements or sensor readings.</p> <p>Key Features</p> <ul> <li>Representation of time series data with optional frequency information</li> <li>Methods for statistical analysis, visualization, and manipulation</li> <li>Support for resampling and filtering operations</li> <li>Support for minmax, standard, and PCA transformations</li> </ul> <p>Usage Example</p> <pre><code>from embdata import Trajectory\nimport numpy as np\n\n# Create a Trajectory\ndata = np.random.rand(100, 3)  # 100 timesteps, 3 dimensions\ntraj = Trajectory(data, freq_hz=10)\n\n# Compute statistics\nstats = traj.stats()\nprint(stats)\n\n# Plot the trajectory\ntraj.plot()\n\n# Resample the trajectory\nresampled_traj = traj.resample(target_hz=5)\n\n# Apply a low-pass filter\nfiltered_traj = traj.low_pass_filter(cutoff_freq=2)\n\n# Save the plot\ntraj.save(\"trajectory_plot.png\")\n</code></pre> <p>Methods</p> <ul> <li><code>stats()</code>: Computes statistics for the trajectory</li> <li><code>plot()</code>: Plots the trajectory</li> <li><code>resample(target_hz)</code>: Resamples the trajectory to a new frequency</li> <li><code>low_pass_filter(cutoff_freq)</code>: Applies a low-pass filter to the trajectory</li> <li><code>save(filename)</code>: Saves the trajectory plot to a file</li> <li><code>show()</code>: Displays the trajectory plot</li> <li><code>transform(operation, **kwargs)</code>: Applies a transformation to the trajectory</li> </ul> <p>The <code>Trajectory</code> class offers methods for analyzing, visualizing, and manipulating trajectory data, making it easier to work with time series data in robotics and other applications.</p> Episode <p>The <code>Episode</code> class provides a list-like interface for a sequence of observations, actions, and other data, particularly useful for reinforcement learning scenarios.</p> <p>Key Features</p> <ul> <li>List-like interface for managing sequences of data</li> <li>Methods for appending, iterating, and splitting episodes</li> <li>Support for metadata and frequency information</li> <li>Integration with reinforcement learning workflows</li> </ul> <p>Usage Example</p> <pre><code>from embdata import Episode, Sample\n\n# Create an Episode\nepisode = Episode()\n\n# Add steps to the episode\nepisode.append(Sample(observation=[1, 2, 3], action=0, reward=1))\nepisode.append(Sample(observation=[2, 3, 4], action=1, reward=0))\nepisode.append(Sample(observation=[3, 4, 5], action=0, reward=2))\n\n# Iterate over the episode\nfor step in episode.iter():\n    print(step.observation, step.action, step.reward)\n\n# Split the episode based on a condition\ndef split_condition(step):\n    return step.reward &gt; 0\n\nsplit_episodes = episode.split(split_condition)\n\n# Extract a trajectory from the episode\naction_trajectory = episode.trajectory(field=\"action\", freq_hz=10)\n\n# Visualize 3D geometrical data, images, and graphs with rerun.io\nepisode.show()\n\n# Access episode metadata\nprint(episode.metadata)\nprint(episode.freq_hz)\n</code></pre> <p>Methods</p> <ul> <li><code>append(step)</code>: Adds a new step to the episode</li> <li><code>iter()</code>: Returns an iterator over the steps in the episode</li> <li><code>split(condition)</code>: Splits the episode based on a given condition</li> <li><code>trajectory(field, freq_hz)</code>: Extracts a trajectory from the episode for a specified field</li> <li><code>filter(condition)</code>: Filters the episode based on a given condition</li> </ul> <p>Properties</p> <ul> <li><code>metadata</code>: Additional metadata for the episode</li> <li><code>freq_hz</code>: The frequency of the episode in Hz</li> </ul> <p>The <code>Episode</code> class simplifies the process of working with sequential data in reinforcement learning and other time-series applications.</p> Pose6D <p>The <code>Pose6D</code> class represents absolute coordinates for a 6D pose in 3D space, including position and orientation.</p> <p>Key Features</p> <ul> <li>Representation of 3D pose with position (x, y) and orientation (theta)</li> <li>Conversion between different units (meters, centimeters, radians, degrees)</li> <li>Conversion to different formats (list, dict)</li> </ul> <p>Usage Example</p> <pre><code>from embdata.geometry import Pose6D\nimport math\n\n# Create a Pose3D instance\npose = Pose6D(x=1.0, y=2.0, z=3.0, roll=math.pi/10, pitch=math.pi/5, yaw=math.pi/3)\n\n# Convert to different units\npose_cm = pose.to(\"cm\")\nprint(pose_cm)  # Pose6D(x=100.0, y=200.0, z=300.0, roll=0.3141592653589793, pitch=0.6283185307179586, yaw=1.0471975511965976)\n\npose_deg = pose.to(angular_unit=\"deg\")\nprint(pose_deg)  # Pose6D(x=1.0, y=2.0, z=3.0, roll=5.729577951308232, pitch=11.459155902616465, yaw=17.374763072956262)\n\n# Convert to different formats\npose_list = pose.numpy()\nprint(pose_list)  # array([1.0, 2.0, 3.0, 0.1, 0.2, 0.3])\n\npose_dict = pose.dict()\nprint(pose_dict)  # {'x': 1.0, 'y': 2.0, 'z': 3.0, 'roll': 0.1, 'pitch': 0.2, 'yaw': 0.3}\n\npose.to(\"quaternion\")\nprint(pose.quaternion())  # [0.9659258262890683, 0.0, 0.13052619222005157, 0.0]\n\npose.to(\"rotation_matrix\")\nprint(pose.rotation_matrix())  # array([[ 0.8660254, -0.25, 0.4330127], [0.4330127, 0.75, -0.5], [-0.25, 0.61237244, 0.75]]\n</code></pre> <p>Methods</p> <ul> <li><code>to(container_or_unit, unit, angular_unit)</code>: Converts the pose to different units or formats</li> </ul> <p>The<code>Pose3D</code> class provides methods for converting between different units and representations of 3D poses, making it easier to work with spatial data in various contexts.</p> HandControl <p>The <code>HandControl</code> class represents an action for a 7D space, including the pose of a robot hand and its grasp state.</p> <p>Key Features</p> <ul> <li>Representation of robot hand pose and grasp state</li> <li>Integration with other motion control classes</li> <li>Support for complex nested structures</li> </ul> <p>Usage Example</p> <pre><code>from embdata.geometry import Pose\nfrom embdata.motion.control import HandControl\n\n# Create a HandControl instance\nhand_control = HandControl(\n    pose=Pose(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n    grasp=0.5\n)\n\n# Access and modify the hand control\nprint(hand_control.pose.position)  # [0.1, 0.2, 0.3]\nhand_control.grasp = 0.8\nprint(hand_control.grasp)  # 0.8\n\n# Example with complex nested structure\nfrom embdata.motion import Motion\nfrom embdata.motion.fields import VelocityMotionField\n\nclass RobotControl(Motion):\n    hand: HandControl\n    velocity: float = VelocityMotionField(default=0.0, bounds=[0.0, 1.0])\n\nrobot_control = RobotControl(\n    hand=HandControl(\n        pose=Pose(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n        grasp=0.5\n    ),\n    velocity=0.3\n)\n\nprint(robot_control.hand.pose.position)  # [0.1, 0.2, 0.3]\nprint(robot_control.velocity)  # 0.3\n</code></pre> <p>Attributes</p> <ul> <li><code>pose</code>: The pose of the robot hand (Pose object)</li> <li><code>grasp</code>: The openness of the robot hand (float, 0 to 1)</li> </ul> <p>The <code>HandControl</code> class allows for easy manipulation and representation of robot hand controls in a 7D space, making it useful for robotics and motion control applications.</p>"},{"location":"#license","title":"License","text":"<p><code>embdata</code> is distributed under the terms of the apache-2.0 license.</p>"},{"location":"#design-decisions","title":"Design Decisions","text":"<ul> <li>Grasp value is [-1, 1] so that the default value is 0.</li> <li>Motion rather than Action to distinguish from non-physical actions.</li> <li>Flattened structures omit full paths to enable dataset transfer between different structures.</li> </ul>"},{"location":"#classes","title":"Classes","text":"Episode <p>The <code>Episode</code> class provides a list-like interface for a sequence of observations, actions, and/or other data. It's designed to streamline exploratory data analysis and manipulation of time series data.</p> <p>Key features</p> <ul> <li>List-like interface for managing sequences of data</li> <li>Methods for appending, iterating, and splitting episodes</li> <li>Support for metadata and frequency information</li> <li>Integration with reinforcement learning workflows</li> </ul> <p>Usage example</p> <pre><code>from embdata import Sample\n\nfrom embdata import Episode, Sample\n\n# Create an Episode\nepisode = Episode()\n\n# Add steps to the episode\nepisode.append(Sample(observation=[1, 2, 3], action=0, reward=1))\nepisode.append(Sample(observation=[2, 3, 4], action=1, reward=0))\nepisode.append(Sample(observation=[3, 4, 5], action=0, reward=2))\n\n# Iterate over the episode\nfor step in episode.iter():\n    print(f\"Observation: {step.observation}, Action: {step.action}, Reward: {step.reward}\")\n\n# Split the episode based on a condition\ndef split_condition(step):\n    return step.reward &gt; 0\n\nsplit_episodes = episode.split(split_condition)\n\n# Extract a trajectory from the episode\naction_trajectory = episode.trajectory(field=\"action\", freq_hz=10)\n\n# Access episode metadata\nprint(episode.metadata)\nprint(episode.freq_hz)\n</code></pre> <p>Methods</p> <ul> <li><code>append(step)</code>: Adds a new step to the episode</li> <li><code>iter()</code>: Returns an iterator over the steps in the episode</li> <li><code>split(condition)</code>: Splits the episode based on a given condition</li> <li><code>trajectory(field, freq_hz)</code>: Extracts a trajectory from the episode for a specified field</li> <li><code>filter(condition)</code>: Filters the episode based on a given condition</li> </ul> <p>Properties</p> <ul> <li><code>metadata</code>: Additional metadata for the episode</li> <li><code>freq_hz</code>: The frequency of the episode in Hz</li> </ul> <p>The <code>Episode</code> class simplifies the process of working with sequential data in reinforcement learning and other time-series applications.</p> Image <p>The <code>Image</code> class represents an image sample that can be represented in various formats, including NumPy arrays, base64 encoded strings, file paths, PIL Images, or URLs.</p> <p>Key Features</p> <ul> <li>Multiple representation formats (NumPy array, base64, file path, PIL Image, URL)</li> <li>Easy conversion between different image formats</li> <li>Resizing and encoding capabilities</li> <li>Integration with other data processing pipelines</li> </ul> <p>Usage Example</p> <pre><code>from embdata import Image\nimport numpy as np\n\n# Create an Image from a numpy array\narray_data = np.random.rand(100, 100, 3)\nimg = Image(array=array_data)\n\n# Convert to base64\nbase64_str = img.base64\n\n# Open an image from a file\nimg_from_file = Image.open(\"path/to/image.jpg\")\n\n# Resize the image\nresized_img = Image(img_from_file, size=(50, 50))\n\n# Save the image\nimg.save(\"output_image.png\")\n\n# Create an Image from a base64 string\nbase64_str = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==\"\nimage = Image.from_base64(base64_str, encoding=\"png\", size=(1, 1))\nprint(image.size)  # Output: (1, 1)\n\n# Example with complex nested structure\nnested_data = {\n    \"image\": Image.from_base64(base64_str, encoding=\"png\"),\n    \"metadata\": {\n        \"text\": \"A small red square\",\n        \"tags\": [\"red\", \"square\", \"small\"]\n    }\n}\nprint(nested_data[\"image\"].size)  # Output: (1, 1)\nprint(nested_data[\"metadata\"][\"text\"])  # Output: A small red square\n</code></pre> <p>Methods</p> <ul> <li><code>open(path)</code>: Opens an image from a file path</li> <li><code>save(path, encoding, quality)</code>: Saves the image to a file</li> <li><code>show()</code>: Displays the image using matplotlib</li> <li><code>from_base64(base64_str, encoding, size, make_rgb)</code>: Creates an Image instance from a base64 string</li> </ul> <p>Properties</p> <ul> <li><code>array</code>: The image as a NumPy array</li> <li><code>base64</code>: The image as a base64 encoded string</li> <li><code>path</code>: The file path of the image</li> <li><code>pil</code>: The image as a PIL Image object</li> <li><code>url</code>: The URL of the image</li> <li><code>size</code>: The size of the image as a (width, height) tuple</li> <li><code>encoding</code>: The encoding format of the image</li> </ul> <p>The <code>Image</code> class provides a convenient interface for working with image data in various formats and performing common image operations.</p> Sample <p>The <code>Sample</code> class is a base model for serializing, recording, and manipulating arbitrary data. It provides a flexible and extensible way to handle complex data structures, including nested objects, arrays, and various data types.</p> <p>Key Features</p> <ul> <li>Serialization and deserialization of complex data structures</li> <li>Flattening and unflattening of nested structures</li> <li>Conversion between different formats (e.g., dict, numpy arrays, torch tensors)</li> <li>Integration with machine learning frameworks and gym spaces</li> </ul> <p>Usage Example</p> <pre><code>from embdata import Sample\nimport numpy as np\n\n# Create a simple Sample instance\nsample = Sample(x=1, y=2, z={\"a\": 3, \"b\": 4}, extra_field=5)\n\n# Flatten the sample\nflat_sample = sample.flatten()\nprint(flat_sample)  # Output: [1, 2, 3, 4, 5]\n\n# Get the schema\nschema = sample.schema()\nprint(schema)\n\n# Unflatten a list back to a Sample instance\nunflattened_sample = Sample.unflatten(flat_sample, schema)\nprint(unflattened_sample)  # Output: Sample(x=1, y=2, z={'a': 3, 'b': 4}, extra_field=5)\n\n# Create a complex nested structure\nnested_sample = Sample(\n    image=Sample(\n        data=np.random.rand(32, 32, 3),\n        metadata={\"format\": \"RGB\", \"size\": (32, 32)}\n    ),\n    text=Sample(\n        content=\"Hello, world!\",\n        tokens=[\"Hello\", \",\", \"world\", \"!\"],\n        embeddings=np.random.rand(4, 128)\n    ),\n    labels=[\"greeting\", \"example\"]\n)\n\n# Get the schema of the nested structure\nnested_schema = nested_sample.schema()\nprint(nested_schema)\n</code></pre> <p>Methods</p> <ul> <li><code>flatten(output_type=\"list\", non_numerical=\"allow\", ignore=None, sep=\".\", to=None)</code>: Flattens the Sample instance into a one-dimensional structure</li> <li><code>unflatten(one_d_array_or_dict, schema=None)</code>: Unflattens a one-dimensional array or dictionary into a Sample instance</li> <li><code>to(container)</code>: Converts the Sample instance to a different container type</li> <li><code>schema(include=\"simple\")</code>: Get a simplified JSON schema of the data</li> <li><code>space()</code>: Return the corresponding Gym space for the Sample instance</li> <li><code>random_sample()</code>: Generate a random Sample instance based on its attributes</li> </ul> <p>The <code>Sample</code> class provides a wide range of functionality for data manipulation, conversion, and integration with various libraries and frameworks.</p> Trajectory <p>The <code>Trajectory</code> class represents a trajectory of steps, typically used for time series of multidimensional data such as robot movements or sensor readings.</p> <p>Key Features</p> <ul> <li>Representation of time series data with optional frequency information</li> <li>Methods for statistical analysis, visualization, and manipulation</li> <li>Support for resampling and filtering operations</li> <li>Transformation and normalization capabilities</li> </ul> <p>Usage Example</p> <pre><code>import numpy as np\nfrom embdata import Trajectory\n\n# Create a simple 2D trajectory\nsteps = np.array([[0, 0], [1, 1], [2, 0], [3, 1], [4, 0]])\ntraj = Trajectory(steps, freq_hz=10, dim_labels=['X', 'Y'])\n\n# Plot the trajectory\ntraj.plot().show()\n\n# Compute and print statistics\nprint(traj.stats())\n\n# Apply a low-pass filter\nfiltered_traj = traj.low_pass_filter(cutoff_freq=2)\nfiltered_traj.plot().show()\n\n# Upsample with rotation splines and bicubic interpolation\nupsampled_traj = traj.resample(target_hz=20)\nprint(upsampled_traj) # Output: Trajectory(steps=..., freq_hz=20, dim_labels=['X', 'Y'])\n\n# Access data\nprint(traj.array)  # Output: [[0 0] [1 1] [2 0] [3 1] [4 0]]\n\n# Get statistics\nstats = traj.stats()\nprint(stats.mean)  # Output: [2. 0.4]\nprint(stats.std)   # Output: [1.41421356 0.48989795]\n\n# Slice the trajectory\nsliced_traj = traj[1:4]\nprint(sliced_traj.array)  # Output: [[1 1] [2 0] [3 1]]\n\n# Transform the trajectory\nnormalized_traj = traj.transform('minmax')\nnormalized_traj.plot().show()\n</code></pre> <p>Methods</p> <ul> <li><code>plot()</code>: Plot the trajectory</li> <li><code>stats()</code>: Compute statistics for the trajectory</li> <li><code>low_pass_filter(cutoff_freq)</code>: Apply a low-pass filter to the trajectory</li> <li><code>resample(target_hz)</code>: Resample the trajectory to a new frequency</li> <li><code>make_relative()</code>: Convert the trajectory to relative actions</li> <li><code>make_absolute(initial_state)</code>: Convert relative actions to absolute actions</li> <li><code>frequencies()</code>: Plot the frequency spectrogram of the trajectory</li> <li><code>frequencies_nd()</code>: Plot the n-dimensional frequency spectrogram of the trajectory</li> <li><code>transform(operation, **kwargs)</code>: Apply a transformation to the trajectory</li> <li><code>make_minmax(min, max)</code>: Apply min-max normalization</li> <li><code>make_pca(whiten)</code>: Apply PCA transformation</li> <li><code>make_standard()</code>: Apply standard normalization</li> <li><code>make_unminmax(orig_min, orig_max)</code>: Reverse min-max normalization</li> <li><code>make_unstandard(mean, std)</code>: Reverse standard normalization</li> <li><code>q01(), q99()</code>: Get 1st and 99th percentiles</li> <li><code>mean(), variance(), std(), skewness(), kurtosis()</code>: Statistical measures</li> <li><code>min(), max()</code>: Minimum and maximum values</li> <li><code>lower_quartile(), median(), upper_quartile()</code>: Quartile values</li> <li><code>non_zero_count(), zero_count()</code>: Count non-zero and zero values</li> </ul> <p>Properties</p> <ul> <li><code>array</code>: The trajectory data as a NumPy array</li> <li><code>freq_hz</code>: The frequency of the trajectory in Hz</li> <li><code>time_idxs</code>: The time index of each step in the trajectory</li> <li><code>dim_labels</code>: The labels for each dimension of the trajectory</li> </ul> <p>The <code>Trajectory</code> class offers comprehensive methods for analyzing, visualizing, manipulating, and transforming trajectory data, making it easier to work with time series data in robotics and other applications.</p> Motion <p>The <code>Motion</code> class is a base class for defining motion-related data structures. It extends the Coordinate class and provides a foundation for creating motion-specific data models.</p> <p>Key Features</p> <ul> <li>Base class for motion-specific data models</li> <li>Integration with MotionField and its variants for proper validation and type checking</li> <li>Support for defining bounds and motion types</li> </ul> <p>Usage Example</p> <pre><code>from embdata.motion import Motion\nfrom embdata.motion.fields import VelocityMotionField\n\nclass Twist(Motion):\n    x: float = VelocityMotionField(default=0.0, bounds=[-1.0, 1.0])\n    y: float = VelocityMotionField(default=0.0, bounds=[-1.0, 1.0])\n    z: float = VelocityMotionField(default=0.0, bounds=[-1.0, 1.0])\n    roll: float = VelocityMotionField(default=0.0, bounds=[\"-pi\", \"pi\"])\n    pitch: float = VelocityMotionField(default=0.0, bounds=[\"-pi\", \"pi\"])\n    yaw: float = VelocityMotionField(default=0.0, bounds=[\"-pi\", \"pi\"])\n\n# Create a Twist motion\ntwist = Twist(x=0.5, y=-0.3, z=0.1, roll=0.2, pitch=-0.1, yaw=0.8)\n\nprint(twist)  # Output: Twist(x=0.5, y=-0.3, z=0.1, roll=0.2, pitch=-0.1, yaw=0.8)\n\n# Access individual fields\nprint(twist.x)  # Output: 0.5\n\n# Validate bounds\ntry:\n    invalid_twist = Twist(x=1.5)  # This will raise a ValueError\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n# Example with complex nested structure\nclass RobotMotion(Motion):\n    twist: Twist\n    gripper: float = VelocityMotionField(default=0.0, bounds=[0.0, 1.0])\n\nrobot_motion = RobotMotion(\n    twist=Twist(x=0.2, y=0.1, z=0.0, roll=0.0, pitch=0.0, yaw=0.1),\n    gripper=0.5\n)\nprint(robot_motion)\n# Output: RobotMotion(twist=Twist(x=0.2, y=0.1, z=0.0, roll=0.0, pitch=0.0, yaw=0.1), gripper=0.5)\n</code></pre> <p>Methods</p> <ul> <li><code>validate_shape()</code>: Validates the shape of the motion data</li> </ul> <p>Fields</p> <ul> <li><code>MotionField</code>: Creates a field for a motion with specified properties</li> <li><code>AbsoluteMotionField</code>: Field for an absolute motion</li> <li><code>RelativeMotionField</code>: Field for a relative motion</li> <li><code>VelocityMotionField</code>: Field for a velocity motion</li> <li><code>TorqueMotionField</code>: Field for a torque motion</li> <li><code>AnyMotionField</code>: Field for any other type of motion</li> </ul> <p>Key Concepts</p> <ul> <li>Subclasses of Motion should define their fields using MotionField or its variants (e.g., AbsoluteMotionField, VelocityMotionField) to ensure proper validation and type checking.</li> <li>The Motion class does not allow extra fields and enforces validation of motion type, shape, and bounds.</li> <li>It can handle various types of motion data, including nested structures with images and text, as long as they are properly defined using the appropriate MotionFields.</li> </ul> <p>The <code>Motion</code> class provides a flexible foundation for creating motion-specific data models with built-in validation and type checking, making it easier to work with complex motion data in robotics and other applications.</p> AnyMotionControl <p>The <code>AnyMotionControl</code> class is a subclass of Motion that allows for arbitrary fields with minimal validation. It's designed for motion control with flexible structure.</p> <p>Key Features</p> <ul> <li>Allows arbitrary fields</li> <li>Minimal validation compared to Motion</li> <li>Includes optional names and joints fields</li> </ul> <p>Usage Example</p> <pre><code>from embdata.motion import AnyMotionControl\n\n# Create an AnyMotionControl instance\ncontrol = AnyMotionControl(names=[\"shoulder\", \"elbow\", \"wrist\"], joints=[0.1, 0.2, 0.3])\nprint(control)  # Output: AnyMotionControl(names=['shoulder', 'elbow', 'wrist'], joints=[0.1, 0.2, 0.3])\n\n# Add arbitrary fields\ncontrol.extra_field = \"some value\"\nprint(control.extra_field)  # Output: some value\n\n# Validation example\ntry:\n    invalid_control = AnyMotionControl(names=[\"joint1\", \"joint2\"], joints=[0.1, 0.2, 0.3])\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n</code></pre> <p>Methods</p> <ul> <li><code>validate_joints()</code>: Validates that the number of joints matches the number of names and that all joints are numbers</li> </ul> <p>Fields</p> <ul> <li><code>names</code>: Optional list of joint names</li> <li><code>joints</code>: Optional list of joint values</li> </ul> <p>The <code>AnyMotionControl</code> class provides a flexible structure for motion control data with minimal constraints, allowing for easy integration with various robotic systems and control schemes.</p> HandControl <p>The <code>HandControl</code> class represents an action for a 7D space, including the pose of a robot hand and its grasp state.</p> <p>Key Features</p> <ul> <li>Representation of robot hand pose and grasp state</li> <li>Integration with other motion control classes</li> <li>Support for complex nested structures</li> </ul> <p>Usage Example</p> <pre><code>from embdata.geometry import Pose\nfrom embdata.motion.control import HandControl\n\n# Create a HandControl instance\nhand_control = HandControl(\n    pose=Pose(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n    grasp=0.5\n)\n\n# Access and modify the hand control\nprint(hand_control.pose.position)  # Output: [0.1, 0.2, 0.3]\nhand_control.grasp = 0.8\nprint(hand_control.grasp)  # Output: 0.8\n\n# Example with complex nested structure\nfrom embdata.motion import Motion\nfrom embdata.motion.fields import VelocityMotionField\n\nclass RobotControl(Motion):\n    hand: HandControl\n    velocity: float = VelocityMotionField(default=0.0, bounds=[0.0, 1.0])\n\nrobot_control = RobotControl(\n    hand=HandControl(\n        pose=Pose(position=[0.1, 0.2, 0.3], orientation=[0, 0, 0, 1]),\n        grasp=0.5\n    ),\n    velocity=0.3\n)\n\nprint(robot_control.hand.pose.position)  # Output: [0.1, 0.2, 0.3]\nprint(robot_control.velocity)  # Output: 0.3\n</code></pre> <p>Attributes</p> <ul> <li><code>pose</code> (Pose): The pose of the robot hand, including position and orientation.</li> <li><code>grasp</code> (float): The openness of the robot hand, ranging from 0 (closed) to 1 (open).</li> </ul> <p>The <code>HandControl</code> class allows for easy manipulation and representation of robot hand controls in a 7D space, making it useful for robotics and motion control applications. It can be integrated into more complex control structures and supports nested data representations.</p> AbsoluteHandControl <p>The <code>AbsoluteHandControl</code> class represents an action for a 7D space with absolute positioning, including the pose of a robot hand and its grasp state.</p> <p>Attributes</p> <ul> <li><code>pose</code> (Pose): The absolute pose of the robot hand, including position and orientation.</li> <li><code>grasp</code> (float): The openness of the robot hand, ranging from -1 (closed) to 1 (open).</li> </ul> <p>The <code>AbsoluteHandControl</code> class provides a straightforward way to define and manipulate absolute hand control actions in a 7D space, making it suitable for precise robotics applications and motion control tasks.</p> RelativePoseHandControl <p>The <code>RelativePoseHandControl</code> class represents an action for a 7D space with relative positioning for the pose and absolute positioning for the grasp.</p> <p>Attributes</p> <ul> <li><code>pose</code> (Pose): The relative pose of the robot hand, including position and orientation.</li> <li><code>grasp</code> (float): The openness of the robot hand, ranging from -1 (closed) to 1 (open).</li> </ul> <p>The <code>RelativePoseHandControl</code> class provides a flexible way to define and manipulate hand control actions with relative pose adjustments and absolute grasp states, making it suitable for various robotics applications and motion control tasks.</p> HeadControl <p>The <code>HeadControl</code> class represents the control for a robot's head movement.</p> <p>Attributes</p> <ul> <li><code>tilt</code> (float): Tilt of the robot head in radians (down is negative).</li> <li><code>pan</code> (float): Pan of the robot head in radians (left is negative).</li> </ul> <p>The <code>HeadControl</code> class provides a straightforward way to define and manipulate head movement controls for a robot, making it useful for various robotics applications and motion control tasks.</p> MobileSingleHandControl <p>The <code>MobileSingleHandControl</code> class represents control for a robot that can move its base in 2D space with a 6D EEF control and grasp.</p> <p>Attributes</p> <ul> <li><code>base</code> (PlanarPose | None): Location of the robot on the ground.</li> <li><code>hand</code> (HandControl | None): Control for the robot hand.</li> <li><code>head</code> (HeadControl | None): Control for the robot head.</li> </ul> <p>The <code>MobileSingleHandControl</code> class provides a comprehensive way to control a mobile robot's base, hand, and head, making it suitable for complex robotics applications requiring coordinated movement and manipulation.</p> MobileSingleArmControl <p>The <code>MobileSingleArmControl</code> class represents control for a robot that can move in 2D space with a single arm.</p> <p>Attributes</p> <ul> <li><code>base</code> (PlanarPose | None): Location of the robot on the ground.</li> <li><code>arm</code> (NumpyArray | None): Control for the robot arm.</li> <li><code>head</code> (HeadControl | None): Control for the robot head.</li> </ul> <p>The <code>MobileSingleArmControl</code> class provides a comprehensive way to control a mobile robot's base, arm, and head, making it suitable for complex robotics applications requiring coordinated movement and manipulation.</p> MobileBimanualArmControl <p>The <code>MobileBimanualArmControl</code> class represents control for a robot that can move in 2D space with two arms.</p> <p>Attributes</p> <ul> <li><code>base</code> (PlanarPose | None): Location of the robot on the ground.</li> <li><code>left_arm</code> (NumpyArray |&gt;None): Control for the left robot arm.</li> <li><code>right_arm</code> (NumpyArray | None): Control for the right robot arm.</li> <li><code>head</code> (HeadControl | None): Control for the robot head.</li> </ul> <p>The <code>MobileBimanualArmControl</code> class provides a comprehensive way to control a mobile robot's base, left arm, right arm, and head, making it suitable for complex robotics applications requiring coordinated movement and manipulation with both arms.</p> HumanoidControl <p>The <code>HumanoidControl</code> class represents control for a humanoid robot.</p> <p>Attributes</p> <ul> <li><code>left_arm</code> (NumpyArray | None): Control for the left robot arm.</li> <li><code>right_arm</code> (NumpyArray | None): Control for the right robot arm.</li> <li><code>left_leg</code> (NumpyArray | None): Control for the left robot leg.</li> <li><code>right_leg</code> (NumpyArray | None): Control for the right robot leg.</li> <li><code>head</code> (HeadControl | None): Control for the robot head.</li> </ul> <p>The <code>HumanoidControl</code> class provides a comprehensive way to control a humanoid robot's arms, legs, and head, making it suitable for complex robotics applications requiring coordinated movement and manipulation.</p>"},{"location":"#data-manupulation","title":"Data Manupulation","text":"<ul> <li> Flattening: Flattening is the process of converting a nested data structure into a one-dimensional array or dictionary.     <ul> <li>A dimension is with respect to a data structure of interest. For example, 1D can be a list of dictionaries where that dictionary structure represents a single dimension.     </li> </ul> </li> <li> Nesting: Loosely defined as \"structures not in the same list\".</li> </ul> <p>A nesting divides substructures by a boundary defined as \"not sharing a list for an ancestor\". For example, in a list of dictionaries, each dictionary is a separate nesting. All substructures within a dictionary are part of the same nesting so long as they are not part of different nestings in a list.</p>"},{"location":"#rl-definitions","title":"RL Definitions","text":"Definitions Used by this Library <ul> <li>Transition: A transition is a tuple (s, a, r, s') where s is the current state, a is the action taken, r is the reward received, and s' is the next state.</li> <li>Episode: An episode is a sequence of transitions that starts at the initial state and ends at a terminal state.</li> <li>Trajectory: A trajectory is a sequence of states, actions, and rewards that represents the behavior of an agent over time.</li> <li>Policy: A policy is a mapping from states to actions that defines the behavior of an agent.</li> <li>Value Function: A value function estimates the expected return from a given state or state-action pair under a given policy.</li> <li>Q-Function: A Q-function estimates the expected return from a given state-action pair under a given policy.</li> <li>Reward Function: A reward function defines the reward received by an agent for taking a particular action in a particular state.</li> <li>Model: A model predicts the next state and reward given the current state and action.</li> <li>Environment: An environment is the external system with which an agent interacts, providing states, actions, and rewards.</li> <li>Agent: An agent is an entity that interacts with an environment to achieve a goal, typically by learning a policy or value function.</li> <li>State: A state is a representation of the environment at a given point in time, typically including observations, measurements, or other data.</li> <li>Exploration: Exploration is the process of selecting actions to gather information about the environment and improve the agent's policy or value function.</li> <ul>"}]}